//	created by Martin Rupp
//	martin.rupp@gcsc.uni-frankfurt.de
//	y12 m02 d29

/** \page pageUG4SetupParallel Setting up ug4 on Parallel Computers / Clusters

- \ref secGeneral_Notes
- \ref secCMake
- \ref secInstallation_of_additional_software 
- \ref secMacOSX
- \ref secCekon
- \ref secJuGene
- \ref secHermit
- \ref secSSH (\ref secSSHKeys, \ref secSSHHoping, \ref secSSHFS) 


<hr>
\section secGeneral_Notes General Notes

All examples are for running ug in parallel with <tt>&lt;NP&gt;</tt> processors 
and <tt>$UGARGS</tt> as arguments, where <tt>&lt;NP&gt;</tt> is a placeholder 
for the number of MPI processes and <tt>$UGARGS</tt> is an Unix environment 
variable which for example is defined by (Bash syntax; just to shorten command 
lines):
\verbatim
UGARGS="-ex ../scripts/tests/modular_scalability_test.lua -dim 2 -grid ../data/grids/unit_square_01/unit_square_01_quads_8x8.ugx"
\endverbatim

\warning Except for your own computer/workstation or explicitly stated, do 
<strong>NOT EVER</strong> use <tt>mpirun -np &lt;NP&gt; ugshell $UGARGS</tt> to 
start your job on a cluster!
The node you are logging into is only a login node, and you don't want to run 
your job on these.


<hr>
\section secCMake CMake, Toolchains, Compilers

\subsection secCMake_ToolchainFiles Toolchain File
On some systems (especially when the software is built for a different system 
than the one which does the build) it is necessary to change some configuration 
settings done by \em CMake (like compilers or flags to use) by a so called 
"toolchain file" (cf. for example 
<a href="http://www.vtk.org/Wiki/CMake_Cross_Compiling">CMake Cross Compiling</a>).

In this case run \em CMake like this
\verbatim
cmake -DCMAKE_TOOLCHAIN_FILE=<TOOLCHAINFILE> ..
\endverbatim


\subsection secCMake_OtherCompilers Setting compilers to use
You can specify other compilers than detected by \em CMake from the command 
line with
\verbatim
cmake -DCMAKE_C_COMPILER=cc -DCMAKE_CXX_COMPILER=CC ..
\endverbatim

\subsection secCMake_GCC412 GCC 4.1.2
<em>GCC v. 4.1.2</em>, like it is default on \em cekon, is not able to compile 
a debug build (i.e. configured with <tt>cmake -DDEBUG=ON ..</tt>) because of an 
internal compiler error (<tt>internal compiler error: in force_type_die, at 
dwarf2out.c...</tt>).
In this case it is possible to configure \ug4 by specifying <em>GCC v. 
4.4.4</em>, which is also installed on \em cekon as an alternative compiler:
\verbatim
cmake -DCMAKE_CXX_COMPILER=/usr/bin/g++44 ..
\endverbatim

Alternatively one can instruct the (default) compiler to produce the debug 
infos in <a href="http://gcc.gnu.org/ml/gcc/2001-04/msg01028.html">another 
format</a>. 
To do so, call \em CMake with
\verbatim
cmake -DDEBUG_FORMAT=-gstabs ..
\endverbatim

This sets \em STABS as format of debug infos.

If you need to choose another compiler, please consider writing your own 
toolchain file, so others can benefit from your knowledge. 


<hr>
\section secInstallation_of_additional_software Installation of Additional Software

Unfortunately on some systems it turned out that especially the build tool 
\em CMake, absolutely necessary to configure \ug4 (cf. \ref pageInstallUG4, 
\ref secInstallUG4CMake), was not available.
In such cases you have to install the required software yourself (typically 
locally).
For some installation instructions &mdash; including those for \em CMake &mdash;
see \ref pageAdditionalSoftware .


<hr>
\section secWindows Windows
<!--
TODO: Write installation instructions for Windows clusters and parallel computing on Windows
-->


<hr>
\section secMacOSX MacOSX

After installation of a MPI implementation (e.g. 
<a href="http://www.open-mpi.org/">OpenMPI</a>, for example via 
<a href="http://www.macports.org">MacPorts</a>:
<tt>sudo port install openmpi</tt>) you can use
\verbatim
mpirun -np <NP> ugshell $UGARGS
\endverbatim
to run \ug4.


<hr>
\section secCekon Cekon

\em Cekon is the in-house cluster of the \em G-CSC.
By now it consists of 23 compute nodes with 4 cores per node, that is 92 
computing cores.

<ul>
  <li>Configuration:
    Normally a run of \em CMake with "standard" parameters should do the job.

    There are some problems with the pre-installed <em>GCC 4.1.2</em>, see 
    \ref secCMake_GCC412 .
    You can also use <tt>icc</tt> as compiler.
  </li>
  <li>The <em>Job Scheduler</em> on \em Cekon is supported by \c ugsubmit / 
    \c uginfo / \c ugcancel (\ref pageugsubmit).

    If you want to do job scheduling manually:<br>
    Start your job with
    \verbatim
salloc -n <NP> mpirun ugshell $UGARGS
    \endverbatim
    Please note that ony the <tt>salloc</tt> parameter <tt>-n</tt> reserves a 
    number of processes / cores of a job, while <tt>-N</tt> (capital \c N) a 
    number of nodes.
    See the <tt>salloc</tt> manual page for further details.
  </li>
  <li>To display information about jobs already running (located in the 
    \em SLURM scheduling queue) use the <tt>squeue</tt> command.
  </li>
  <li>\b Debugging : If \em DDT is installed, simply type <tt>ddt</tt> in the 
    Unix shell to start the Debugger and fill in the job definition of the job 
    to be debugged in the GUI (X11 based) &mdash; everything should be quite 
    self-explanatory.
  </li>
</ul>


<hr>
\section secJuGene JuGene

<a href="http://www.fz-juelich.de/ias/jsc/EN/Expertise/Supercomputers/JUGENE/JUGENE_node.html">JuGene</a>
&mdash; the 
<a href="http://www.fz-juelich.de/ias/jsc/EN/Expertise/Supercomputers/JUGENE/Configuration/Configuration_node.html">72 racks Blue Gene/P</a>
system at <em>J&uuml;lich Supercomputing Centre (JSC)</em> (FZ J&uuml;lich) in total 
provides 294.912 cores (288 Ki) and 144 Tbyte RAM.
The 73.728 compute nodes (CN) each have a quadcore Power 32-bit PC 450 running 
at 850 MHz, with 2 Gbyte of RAM. 

Half a rack (2048 cores) is called a \b midplane.
The \em JuGene system uses five different networks dedicated for various tasks 
and functionalities of the machine.
Relevant for us:
The 3-D torus network.
This is a point-to-point network &mdash; each of the CNs has six 
nearest-neighbour connections.

See more on the architecture of \em JuGene 
<a href="http://www.fz-juelich.de/SharedDocs/Downloads/IAS/JSC/EN/JUGENE/SlidesBGPArchitecture.pdf">here</a>.

More information about \em JuGene "in order to enable users of the system to 
achieve good performance of their applications" can be found in
<a href="http://www.prace-ri.eu/IMG/pdf/Best-practise-guide-JUGENE-v0-3.pdf">"PRACE Best-Practice Guide for JUGENE"</a>.

\note Note that the login nodes are running under <em>SuSE Linux Enterprise 
Server 10</em> (SLES 10), while the CNs are running a limited version of Linux 
called <em>Compute Node Kernel</em> (CNK).
Therefore its necessary to <strong>cross-compile</strong> for \em JuGene (cf. 
sec. \ref secCMake; sec. \ref secConfiguration_of_ug4_for_JuGene).


<hr>
\subsection secAccess_to_JuGenes_login_nodes Access to JuGene's Login Nodes
\em JuGene is reached via two so called <strong>front-end</strong> or 
<strong>login nodes</strong> (\c jugene3 and \c jugene4) for interactive access 
and the submission of batch jobs.

These login nodes are reached via
\verbatim
ssh <user>@jugene.fz-juelich.de
\endverbatim
i.e., for login there is only a generic hostname, <tt>jugene.fz-juelich.de</tt>,
from which automatically a connection either to \c jugene3 or \c jugene4 will 
be established.

The front-end nodes have an identical environment, but multiple sessions of one 
user may reside on different nodes which must be taken into account when 
killing processes.

It is necessary to <strong>upload the SSH key</strong> of the machine from 
which to connect to one of JuGenes login nodes.
See 
<a href="http://www.fz-juelich.de/ias/jsc/EN/Expertise/Supercomputers/JUGENE/UserInfo/Logon.html">Logging on to JUGENE</a>
(also for \em X11 problems).

To be able to connect to \em JuGene from different machines maybe you find it 
useful to define one of \em GCSC's machines (e.g. \c speedo, \c quadruped, ...) 
as a "springboard" to one of \em JuGenes login nodes (so that you have to login 
to this machine first, then to \em JuGene), see \ref secSSHHoping.


<hr>
\subsection secConfiguration_of_ug4_for_JuGene Configuration of ug4 for JuGene
For JuGene you have to "cross compile" and to do so use a specific 
\ref secCMake_ToolchainFiles.
Start CMake like this
\verbatim
cmake -DCMAKE_TOOLCHAIN_FILE=../toolchain_file_jugene.cmake ..
\endverbatim
or, for <strong>static builds</strong> which is the <strong>configuration of 
choice</strong> if you want to <strong>process very large jobs</strong>,
\verbatim
cmake -DSTATIC=ON -DCMAKE_TOOLCHAIN_FILE=../toolchain_file_jugene_static.cmake ..
\endverbatim

See also \ref secVery_large_jobs_on_JuGene!

\note A <em>static build</em> where also all system libraries are linked 
statically need some additional "hand work" by now:
After configuration with \em CMake edit the following files by replacing all 
occurences of <tt>libXXXXXXX.so</tt> by <tt>libXXXXXXX.a</tt>
(has to be done only once):
\verbatim
CMakeCache.txt,
ugbase/ug_shell/CMakeFiles/ugshell.dir/link.txt,
ugbase/ug_shell/CMakeFiles/ugshell.dir/build.make
\endverbatim

Or use this <tt>sed</tt> command:
\verbatim
sed -i '' 's/\([[:alnum:]]*\).so/\1.a/g' CMakeCache.txt ugbase/ug_shell/CMakeFiles/ugshell.dir/link.txt ugbase/ug_shell/CMakeFiles/ugshell.dir/build.make 
\endverbatim

You can check your executable by running the <tt>ldd</tt> command on it:
\verbatim
ldd ugshell
\endverbatim
Answer should be <tt>not a dynamic executable</tt> for a completely static 
build!


<hr>
\subsection secWorking_with_ug4_on_JuGene Working with ug4 on JuGene

\subsubsection secBasic_job_handling Basic job handling

Please take the time and fill out the details in \c ugsubmit / \c uginfo / 
\c ugcancel (\ref pageugsubmit).

See 
<a href="http://www.fz-juelich.de/ias/jsc/EN/Expertise/Supercomputers/JUGENE/UserInfo/QuickIntroduction.html">Quick Introduction</a>
to job handling.
Also look at 
<a href="http://www.fz-juelich.de/ias/jsc/EN/Expertise/Supercomputers/JUGENE/UserInfo/mpirun.html">job/mpirun options</a>.

Here we only introduce some important details (everything else should become 
clear by examining the examples provided):
<ul>
  <li><storng>Jobs can be submitted</strong> by the <tt>llrun</tt> 
    (interactively) and the <tt>mpirun</tt> (as batch job; defined in a 
    \em LoadLeveler script) command.
    See below for some details.
  </li>
  <li>There are different <strong>execution modes</strong> which can be 
    specified by the <tt>mpirun</tt> and <tt>llrun</tt> parameter 
    <tt>-mode {VN | DUAL | SMP}</tt>:
    <ul>
      <li><strong>Quad Mode</strong> (a.k.a. "Virtual Mode"):
        All four cores run one MPI process.
        Memory/MPI Process = 1/4 CN RAM:
        <tt>-mode VN</tt>.
      </li>
      <li><strong>Dual Mode</strong>:
        Two cores run one MPI process (hybrid MPI/OpenMP).
        Memory/MPI Process = 1/2 CN RAM:
        <tt>-mode DUAL</tt>.
      </li>
      <li><strong>SMP Mode</strong> ("Symmetrical Multi Processing Mode"):
        All four cores run one MPI process (hybrid MPI/OpenMP).
        Memory/MPI Process = CN RAM:
        <tt>-mode SMP</tt>.
      </li>
    </ul>
    Note that in quad mode (using all 4 processors of a computing node) this 
    means each core has only ca. 512 Mbyte of RAM (474 Mbyte to be more 
    specific, since the CNK also needs some memory).
    
    Obviously "VN" is the preferred execution mode if large numbers of 
    processes should be achieved &mdash; and \ug4 works with VN mode (at least 
    up to ~64 Ki DoFs per process)!
  </li>
  <li>The <tt>mpirun</tt> parameter <tt>-mapfile</tt> specifies an order in 
    which the MPI processes are mapped to the CNs / the cores of the BG/P 
    partition reserved for the run.
    This order can either be specified by a permutation of the letters X,Y,Z 
    and T \em or the name of a mapfile in which the distribution of the tasks 
    is specified, <tt>-mapfile {&lt;mapping&gt;|&lt;mapfile&gt;}</tt>:
    <ul>
      <li><tt>&lt;mapping&gt;</tt> is a permutation of X, Y, Z and T.

        The standard mapping on \em JuGene is to place the tasks in "XYZT" 
        order, where X, Y, and Z are the torus coordinates of the nodes in the 
        partition and T is the number of the cores within each node (T=0,1,2,3).

        When the tasks are distributed across the nodes the first dimension is 
        increased first, i.e. for XYZT the first three tasks would be executed 
        by the nodes with the torus coordinates <tt>&lt;0,0,0,0&gt;</tt>, 
        <tt>&lt;1,0,0,0&gt;</tt> and <tt>&lt;2,0,0,0&gt;</tt>, which obviously 
        is not what we want for our simulation runs.
        For now we recommend <tt>-mapfile TXYZ</tt> which fills up a CN before 
        going to the next CN so that MPI processes working on adjacent 
        subdomains are placed closely in the 3-D torus.
      </li>
      <li><tt>&lt;mapfile&gt;</tt> is the name of a mapfile in which the 
        distribution of the tasks is specified:
        It contains a line of <tt>x y z t</tt> coordinates for each MPI process.
        See sec. 6. of the <em>Best-Practise guide</em> mentioned above for an 
        example and which LoadLeveler keywords to use.
      </li>
    </ul>
<!--
TODO: Some notes about "topology aware placing of MPI processes" might become relevant in the future.
-->
  </li>
  <li>If \ug4 was dynamically linked add to the <tt>mpirun</tt> parameters 
    <tt>-env LD_LIBRARY_PATH==/bgsys/drivers/ppcfloor/comm/lib/</tt>.
    \note This parameter is (obviously) not necessary for completely static builds!

  </li>
  <li><strong>"Modules"</strong> can be loaded by executing the <tt>module</tt> 
    command, e.g. <tt>module load lapack</tt> for LAPACK.

<!--
TODO: Nevertheless CMake says "Info: Not using Lapack. No package found"!?
-->

    This is also for loading <strong>performance analysis tools</strong>, e.g. 
    <tt>module load scalasca</tt>, <tt>module load UNITE</tt> etc.
     
<!--
TODO: Please provide a link which explains a bit more !!!
-->
     
     <!-- no experience with this stuff yet! -->
  </li>
  <li><a href="http://www.fz-juelich.de/ias/jsc/EN/Expertise/Supercomputers/JUGENE/UserInfo/llview.html"><tt>llview</tt></a>
    is a <strong>tool with a graphical X11 user interface</strong> for 
    displaying system status, running jobs, scheduling and prediction of the 
    start time of jobs.
<!--
TODO: llview on JuGene
--> 
    If estimated start times can not determined by 
    <tt>llq -s &lt;job-id&gt;</tt> (see below) this is to our knowledge the 
    only possibility to get this information.
  </li>
  <li><strong>Interactive jobs</strong> can be started with the 
    <a href="http://www.fz-juelich.de/ias/jsc/EN/Expertise/Supercomputers/JUGENE/UserInfo/LoadLevelerInteractive.html">llrun</a>
    command.

    Example:
    \verbatim
llrun -np <NP> -exe ./ugshell -mode VN -mapfile TXYZ -verbose 2 -env LD_LIBRARY_PATH=/bgsys/drivers/ppcfloor/comm/lib/ $UGARGS
    \endverbatim

    Please note that <tt>llrun</tt> only allows jobs up to 256 
    (<tt>-mode SMP</tt>) / 512 (<tt>-mode DUAL</tt>) / 1024 (<tt>-mode VN</tt>) 
    MPI processes!
  </li>
  <li><strong>Batch Jobs</strong> are defined in so called <strong>"LoadLeveler 
    scripts"</strong> and submitted by the 
    <a href="http://www2.fz-juelich.de/jsc/jugene/usage/loadl/llsubmit/"><tt>llsubmit</tt></a>
    command to the <em>IBM Tivoli Workload Scheduler LoadLeveler</em> (TWS 
    LoadLeveler), typically in the directory where the ug4 executable resides:
    \verbatim
llsubmit <cmdfile>
    \endverbatim
    <tt>&lt;cmdfile&gt;</tt> is a (plain Unix) shell script file (i.e., the 
    "LoadLeveler script"), which contains job definitions given by 
    <strong>"LoadLeveler keywords"</strong> (some important examples are 
    explained below).

    If <tt>llsubmit</tt> was able to submit the job it outputs a <strong>job 
    name</strong> (e.g. <tt>jugene4b.zam.kfa-juelich.de.298508</tt>) with which 
    a specific job can be identified in further commands, e.g. to cancel it 
    (see below).

    The <strong>output</strong> of the run (messages from Frontend end Backend 
    MPI &mdash; and the output of \ug4 &mdash; is written to a file in the 
    directory where <tt>llsubmit</tt> was executed and whose name begins with 
    the job name you have specified in the LoadLeveler script and ends with 
    <tt>&lt;job number&gt;.out</tt>.

    For some <strong>example LoadLeveler scripts</strong> used with \ug4 see 
    subdirectory <tt>scripts/shell/</tt>:

    <ul>
      <li><tt>ll_scale_gmg.x</tt> contains job definitions for a complete 
        scalability study for GMG in 2-D and 3-D.
      </li>
      <li><tt>ll_template.x</tt> also contains some documentation of LoadLeveler 
        and mpirun parameters.
      </li>
    </ul>
    (All <tt>mpirun</tt> commands therein are commented out &mdash; to performe 
    a specific run remove the comment sign.)

    Please change in your copy of one of these scripts at least the value of the 
    </tt>notify_user</tt> keyword before submitting a job ...

    See also this 
    <a href="http://www.fz-juelich.de/ias/jsc/EN/Expertise/Supercomputers/JUGENE/UserInfo/LoadLeveler.html">more recent JSC documentation</a>,
    and especially the
    <a href="http://www.fz-juelich.de/ias/jsc/EN/Expertise/Supercomputers/JUGENE/UserInfo/LoadLevelerSamples.html">Job File Samples</a>
    for more details.
  </li>
  <li><strong>LoadLeveler keywords</strong> are strings embedded in comments 
    beginning with the characters "# @".
<!--
TODO: isn't there a better overview on the web?
-->
    Some selected keywords:
    <ul>
      <li><tt>job_type = bluegene</tt> specifies that the job is running on 
        <em>JuGene</em>'s CNs.
      </li>
      <li><tt>job_name</tt> specifies the name of the job, which will get part 
        of the name of the output file.
      </li>
      <li><tt>bg_size</tt> specifies the size of the BG/P partition reserved 
        for the job in <strong>number of compute nodes</strong>.

        That is, for <tt>&lt;NP&gt;</tt> MPI processes, <tt>bg_size</tt> must 
        be >= <tt>(&lt;NP&gt;)/4</tt>.

        Alternatively the size of a job can be defined by the <tt>bg_shape</tt> 
        keyword.

        See comments in the example LoadLeveler scripts for proper settings.
      </li>
      <li><tt>bg_connection</tt> specifies the "connection type", i.e. the 
        <strong>network topology</strong> used by a job.

        Connection type can be one in <tt>[TORUS| MESH | PREFER_TORUS]</tt>.
        Default is <tt>bg_connection = MESH</tt>.

        <tt>bg_connection = TORUS</tt> &mdash; utilising the 3-D torus network 
        &mdash; is the preferred topology for our jobs.
        For this <tt>bg_size</tt> (see above) must be >= 512.

        See also comments and usage in the example LoadLeveler scripts for 
        proper settings.
      </li>
    </ul>
    Please note that keywords must not be followed by comments in the same line! 

    A nice introduction to LoadLeveler command file syntax is given e.g. 
    <a href="https://docs.loni.org/wiki/LoadLeveler_Command_File_Syntax">here</a>.
  </li>
  <li><tt>llq</tt> is used to display the status of jobs (of a specified user) 
    in the queue/executed:
    \verbatim
llq [-u  <userid>]
    \endverbatim
<!--
TODO: (to test)
-->
    The estimated start time of a job can (maybe) also be determined by 
    <tt>llq -s &lt;job-id&gt;-id</tt> (cf. 
    https://docs.loni.org/wiki/Useful_LoadLeveler_Commands).
  </li>
  <li><tt>llcancel</tt> is used to cancel a job (<tt>&lt;jobname&gt;</tt> as 
    displayed by <tt>llq</tt>):
    \verbatim
llcancel <jobname>
    \endverbatim
  </li>
  <li>Debugging:
    See documentation to the tools 
    <tt>/bgsys/drivers/ppcfloor/tools/coreprocessor</tt>, <tt>gdbserver</tt> ...
  </li>
  <li>Available file systems.
    See
    <a href="http://www.fz-juelich.de/ias/jsc/EN/Expertise/Supercomputers/JUDGE/Userinfo/Access_Environment.html">JSC documentation</a>
    for more details.
  </li>
  <li>Querying <strong>Quota Status</strong>:
    \verbatim
q_cpuquota <options>
    \endverbatim

    Useful options: 
    \arg <tt>-?</tt>                   usage information and all options .
    \arg <tt>-j &lt;jobstepid&gt;</tt> for a single job.
    \arg <tt>-t &lt;time&gt;</tt>      for all jobs in the specified time, e.g.
      <tt>q_cpuquota -t 23.11.2011 01.12.2011</tt>.
    \arg <tt>-d &lt;number&gt;</tt> for last number of days (positive integer).
  </li>
</ul>


<hr>
\subsubsection secVery_large_jobs_on_JuGene Very large jobs on JuGene

<ul>
  <li>Although its possible on \em JuGene to create shared libraries and run 
    dynamically linked executables this is <strong>in general not 
    recommended</strong>, since loading of shared libraries can delay the 
    startup of such an application considerably, especially when using large 
    partitions (8 racks or more).
    See also
    <a href="http://www.fz-juelich.de/ias/jsc/EN/Expertise/Supercomputers/JUGENE/UserInfo/SharedLibraries.html">Shared Libraries and Dynamic Executables</a>.

    So, for <strong>very large jobs</strong> be sure to have \ug4 built as a 
    <strong>completely static executable</strong> (c.f. 
    \ref secConfiguration_of_ug4_for_JuGene) since otherwise loading of the 
    shared libraries consumes too much wall time!
  </li>
  <li><strong>Very large jobs</strong> (e.g. jobs larger than 32 racks) 
    normally run on <b>Tuesday</b> only.

    Exceptions to this rule are possible in urgent cases (please contact the SC 
    Support under <a href="sc@fz-juelich.de">sc@fz-juelich.de</a>).
  </li>
</ul>

More information ingo.heppner@gcsc.uni-frankfurt.de .


<hr>
\section secHermit Hermit

\subsection secHermit_Architecture Architecture
The <a href="https://wickie.hlrs.de/platforms/index.php/Cray_XE6">Cray XE6 ("Hermit")</a>.
In Installation step 1, the XE6 is a 3552 node cluster.
Each node is a Dual Socket
<a href="http://en.wikipedia.org/wiki/List_of_AMD_Opteron_microprocessors#Opteron_6200-series_.22Interlagos.22_.2832_nm.29">AMD Opteron 6276 (Interlagos)</a>
@ 2.3GHz 16 cores each, which results in 113.664 cores in total.
Normal nodes have 32 GB RAM, 480 special nodes have 64 GB (total 126 TB).
(<a href="https://wickie.hlrs.de/platforms/index.php/CRAY_XE6_Hardware_and_Architecture"> Architecture</a>)
That is 1 GB RAM for each process when running the maximum of 32 processes on a 
node. 
Current maximum number of cores for one job is 64000.
Speak to the administration for more nodes.

\subsection secHermit_General General 
Using the
<a href="https://wickie.hlrs.de/platforms/index.php/CRAY_XE6_Using_the_Batch_System">Batch system</a>
and the
<a href="https://wickie.hlrs.de/platforms/index.php/Workspace_mechanism">Workspace mechanism</a>.

The Job Scheduler on Hermit is supported by \c ugsubmit / \c uginfo / 
\c ugcancel (\ref pageugsubmit).
You might want to use -Hermit-workspace .

\note You have to choose modules every time you log in (you might want to add 
your <tt>module load/swap</tt> commands into your <tt>.bashrc</tt> or similar).


\subsection secHermit_GCC GCC
First, look what modules are loaded
\verbatim
module list
\endverbatim

There is one which is named PrgEnv-cray or PrgEnv-*. Now you swap that to 
PrgEnv-gnu:
\verbatim
module swap PrgEnv-cray PrgEnv-gnu
\endverbatim

There's a one-liner for this task:
\verbatim
module swap $(module li 2>&1 | awk '/PrgEnv/{print $2}') PrgEnv-gnu
\endverbatim

Then you start cmake with a \ref secCMake_ToolchainFiles :
\verbatim
cmake -DCMAKE_TOOLCHAIN_FILE=../toolchain_file_hermit_gcc.cmake ..
\endverbatim

\subsection secHermit_Cray Cray CC
The Cray C Compiler is not working at the moment because there is an internal 
compiler error in release mode.

Toolchain file is <tt>../toolchain_file_hermit.cmake</tt>, and the module is 
PrgEnv-cray.
\verbatim
module swap $(module li 2>&1 | awk '/PrgEnv/{print $2}') PrgEnv-cray
cmake -DCMAKE_TOOLCHAIN_FILE=../toolchain_file_hermit.cmake ..
\endverbatim

\subsection secHermit_Workspace Workspace Mechanism
Access to the user file system (anything in your home directory) from your 
running job is very slow on Hermit.
It is very noticable in runs with 1024 cores and even if you are accessing only 
small files once like your script files.
The
<a href="https://wickie.hlrs.de/platforms/index.php/Workspace_mechanism">Workspace Mechanism</a>
lets you create a directory on a specialized parallel file system.
Use a script like this to create a workspace directory and copy all you data 
there:
\verbatim
runDir=`ws_allocate $mydate 15`
echo "Hermit Workspace dir is $runDir"
ln -s $runDir $(date "+workspace.%m-%d-%H%M");
cp -r ../scripts $runDir
cp -r ../data $runDir
\endverbatim

Now before you run \ug4, you want \ug4 to know about its new root dir. 
Be aware that also file written to can be damaged if you are not using the 
Workspace mechanism. 
See also the "-dir" and "-Hermit-workspace" option in \ref pageugsubmit.

\verbatim
export UG4_ROOT=$runDir
\endverbatim
Otherwise, \ug4 will look in <tt>../scripts/</tt> for scripts and 
<tt>../data/</tt> for data, relative to the path of the binary.


<hr>
\section secSSH SSH

\subsection secSSHKeys Exchanging SSH Keys
You can exchange RSA keys so you don't have to type in your password every time 
you connect.

<ul> 
  <li>generate local ssh key (empty password):
    \verbatim
ssh-keygen -b 2048 -t rsa
    \endverbatim
  <li>copy local ssh key to cluster.de (user mrupp):
    \verbatim
cat .ssh/id_rsa.pub | ssh mrupp@cluster.de 'umask 077; cat >> .ssh/authorized_keys'
    \endverbatim
  </li>
</ul>

\subsection secSSHHoping SSH Hoping
Many supercomputers are only accessible by some fixed IP you specify.
Since at home we do not have a fixed IP, you have to use SSH hoping.
Say speedo's IP (<tt>speedo.gcsc.uni-frankfurt.de</tt>) is specified as the 
connecting IP, and your account is <tt>mrupp\@speedo.gcsc.uni-frankfurt.de</tt>, 
and <tt>rupp\@supercomputer.de</tt> is the supercomputer account.
Then you can connect to \c speedo via ssh, and from there further with ssh to 
the supercomputer:
\verbatim
ssh mrupp@speedo.gcsc.uni-frankfurt.de
ssh rupp@supercomputer.de
\endverbatim

There are two ways to speed this up
<ul>
  <li>hopping in one line:
    \verbatim
ssh -t mrupp@speedo.gcsc.uni-frankfurt.de ssh rupp@supercomputer.de
    \endverbatim
  </li>
  <li>automatic hopping: Add the following lines to the file 
    <tt>~/.ssh/config</tt>
    \verbatim
Host supercomputer.de
User rupp
ForwardAgent yes
ProxyCommand ssh mrupp@speedo.gcsc.uni-frankfurt.de nc %h %p
    \endverbatim
    now you'll only have to enter
    \verbatim
ssh supercomputer.de
    \endverbatim
    and enter the passwords.
  </li>
</ul>


\subsection secSSHFS FUSE/SSHFS

Manipulating files on remote system can be really tedious.
Fortunately, with SSHFS you can mount filesystems over ssh into your computer.
It's like having a supercomputer as a USB stick. For that

<ul>
  <li>MacOS <= 10.6: download 
    <a href="http://code.google.com/p/macfuse/downloads/list">macfuse</a>.
    install.<br>
    MacOS >= 10.7: use OSX Fuse.
  </li>
  <li>install sshfs:
    \verbatim
cd ~/Downloads/
svn co http://macfuse.googlecode.com/svn/trunk/filesystems/sshfs/binary sshfs-binaries
cd sshfs-binaries/
sudo cpsshfs-static-leopard /usr/bin/sshfs
    \endverbatim
    For tiger, use <tt>sshfs-static-tiger</tt>.
  </li>
</ul>


Now FUSE/SSHFS is installed.
Now we'll mount the home directory of <tt>mrupp\@supercomputer.de</tt> to 
<tt>~/sshvolumes/mrupp\@supercomputer</tt>.
\verbatim
mkdir ~/sshvolumes
mkdir ~/sshvolumes/mrupp@supercomputer
sshfs mrupp@supercomputer.de: ~/sshvolumes/mrupp@supercomputer -o auto_cache,reconnect,volname=mrupp@supercomputer
\endverbatim

unmounting is done via
\verbatim
umount ~/sshvolumes/mrupp@supercomputer
\endverbatim

\note SSHFS also works together with \ref secSSHHoping and \ref secSSHKeys.
For Windows, try <a href="http://dokan-dev.net/en/download/">Dokan library</a>.
<!--
TODO: Add install help for windows/linux
-->

\subsection secSSH_Eclipse SSHFS + Eclipse
Note for \em Eclipse users:
You can also import a project on your mounted filesystem.
For that, create a C/C++ project and as base directory you use the \ug4 base 
directory.
You'll then even get SVN info.
You can also build on the remote server, but don't use the normal build command:
Right Mouse click on the project -> Properties -> C/C++ Build.
Disable "use default build command", then enter at "Build Command" something 
like
\verbatim
ssh mrupp@supercomputer.de "cd ug4; cd release; make -j4"
\endverbatim
(You can do the same for run/debug settings).

An improved version is
\verbatim
/bin/bash ${ProjDirPath}/scripts/shell/sshfsmake mrupp@supercomputer.de /Volumes/mrupp@supercomputer ug4/debug -j4 -k
\endverbatim
See <tt>ug4/scripts/shell/sshfsmake</tt> for details (basically you are calling 
<tt>ssh mrupp\@supercomputer.de "cd ug4/debug; make -j4"</tt>, and then 
substituting remote paths with local ones, so \em Eclipse knows which files 
contain errors).

Since SSHFS is not as fast as your local drive, you might want to disable the 
most bandwith consuming things (at Project Preferences):
- C/C++ Build -> Discovery Options: Disable "Automated discovery of paths and symbols"
- C/C++ Build -> Logging: Disable Logging
- C/C++ General -> Indexer -> Disable Indexer

If that does not help, consider \em Eclipse -> Preferences -> Team -> SVN -> 
Performance, altough deactivating "deep outgoing state" is not recommended.<br>
Remember that \em Eclipse does not know that the files are not on your local 
machine, so when you are doing big SVN checkouts in \em Eclipse, all data is 
going to your computer, and then to the remote machine. 
Consider using the remote shell for those tasks. 

*/
